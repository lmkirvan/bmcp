---
title: "chapter 2"
format: typst
---
 
### bayesian p values 

```{=typst}

$ p Beta = p( attach(T, br:"sim") <= attach(T, br: "obs") | tilde(Y)) $ 


```

### Diagnostics


```{python}

import arviz as az
import matplotlib.pyplot as plt
import numpy as np
import pymc as pm
from scipy import stats

```

```{python}

good_chains = stats.beta.rvs(2, 5, size=(2, 2000))
bad_chains0 = np.random.normal(np.sort(good_chains, axis=None), 0.05,
                               size=4000).reshape(2, -1)

bad_chains1 = good_chains.copy()
for i in np.random.randint(1900, size=4):
    bad_chains1[i%2:,i:i+100] = np.random.beta(i, 950, size=100)

chains = {"good_chains":good_chains,
          "bad_chains0":bad_chains0,
          "bad_chains1":bad_chains1}
```

Effective sample size 


```{python}
import arviz as az

print(az.ess(chains))

```


```{python}
import matplotlib.pyplot as plt

_, axes = plt.subplots(2, 3, figsize=(12, 6), sharey=True, sharex=True)
az.plot_ess(chains, kind="local", ax=axes[0])
az.plot_ess(chains, kind="quantile", ax=axes[1])

for ax_ in axes[0]:
    ax_.set_xlabel("")
for ax_ in axes[1]:
    ax_.set_title("")

for ax_ in axes[:,1:].ravel():
    ax_.set_ylabel("")
plt.ylim(-100, 5000)

```

```{python}

# this doesn't have a shared Y axis by default,
#  probably becauase you normally would normally 
# look at the parameters at basically the same tim
az.plot_mcse(chains)

```


```{python}

az.plot_trace(chains)

```

this is nice it shows chains for both realizations 

```{python}
az.plot_autocorr(chains)

```

I think that this is the best way to look at the chains

```{python}

az.plot_trace(chains, kind="rank_vlines")

```

code 2.12 

```{python}

import pymc as pm 

with pm.Model() as model_0:
    theta1 = pm.Normal("theta1", 0, 1, initval=0.1)
    theta2 = pm.Uniform("theta2", -theta1, theta1)
    idata_0 = pm.sample(return_inferencedata=True) 

```


```{python}

y_obs =  np.random.normal(0, 1, size=100)
idatas_cmp = {}

with pm.Model() as mA:
    σ = pm.HalfNormal("σ", 1)
    y = pm.SkewNormal("y", mu=0, sigma=σ, alpha=1, observed=y_obs)
    idataA = pm.sample(idata_kwargs={"log_likelihood":True})
    idataA.extend(pm.sample_posterior_predictive(idataA))
    idatas_cmp["mA"] = idataA

# zero mean, happens to be correct here 

with pm.Model() as mB:
    σ = pm.HalfNormal("σ", 1)
    y = pm.Normal("y", 0, σ, observed=y_obs)
    idataB = pm.sample(idata_kwargs={"log_likelihood":True})
    idataB.extend(pm.sample_posterior_predictive(idataB))
    idatas_cmp["mB"] = idataB

# random mean 

with pm.Model() as mC:
    μ = pm.Normal("μ", 0, 1)
    σ = pm.HalfNormal("σ", 1)
    y = pm.Normal("y", μ, σ, observed=y_obs)
    idataC = pm.sample(idata_kwargs={"log_likelihood":True})
    idataC.extend(pm.sample_posterior_predictive(idataC))
    idatas_cmp["mC"] = idataC

az.compare(idatas_cmp)    

```


```{python}
az.plot_elpd(idatas_cmp, figsize=(10, 5), plot_kwargs={"marker":"."}, threshold=2);

```

plot shape paremeter K for loo computation (value for pareto distribution) can be used to detect highly influential points (above 0.7 as a rule of thumb, none here)


```{python}
_, axes = plt.subplots(1, 3, figsize=(12, 4), sharey=True)
for idx, (model, ax) in enumerate(zip(("mA", "mB", "mC"), axes)):
    loo_ = az.loo(idatas_cmp[model], pointwise=True)
    az.plot_khat(loo_, ax=ax, threshold=0.09, show_hlines=True, hlines_kwargs={"hlines":0.09, "ls":"--"})
    ax.set_title(model)
    if idx:
        axes[idx].set_ylabel("")
    if not idx % 2:
        axes[idx].set_xlabel("")

```

KDE with data 

```{python}

plt.close()

az.plot_kde(y_obs, rug=True)

```


Loo-pit 

```{=typst}

$ P_i = P(tilde(y_i) <= Y_i | y-i) $ 

```
This should look approximately unifor in distribution. 

```{python}

_, axes = plt.subplots(1, 3, figsize=(12, 4), sharey=True)
for model, ax in zip(("mA", "mB", "mC"), axes):
    az.plot_loo_pit(idatas_cmp[model], y="y", legend=False, use_hdi=True, ax=ax)
    ax.set_title(model)
    ax.set_xticks([0, 0.5, 1])
    ax.set_yticks([0, 1, 2])


```

model averaging with weights: 

- weight with a list of traces and a list of weights 
- model stacking is used in the look compare weights! e


```{python}
    
pm.sample_posterior_predictive_w()     

```

h3


```{python}

c8 = az.load_arviz_data("centered_eight")

az.plot_posterior(c8.posterior, )

az.plot_density(c8.posterior_predictive)

temp = az.hdi(c8.posterior)

az.plot_hdi(c8.posterior)

az.plot_forest(c8)

c8.posterior['tau'].mean()


```


